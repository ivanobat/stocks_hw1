---
title: "homework1"
author: "Ivan Aguilar"
date: '2022-05-03'
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: false
---
# Initial setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:\\Users\\ivano\\Desktop\\DSMMaster\\MLFin\\')
rm(list=ls())

library(tidyverse)
library(xts)
library(quantmod)
library(TTR)
library(lmtest)
library(dygraphs)
library(kernlab)
library(ggplot2)
library(glmnet)

set.seed(666)
```

```{r}
# Auxiliary functions
ssr <-function(actual,pred){
  sum((actual - pred)^2)
}

##Normalize Residual Mean Square Error (NRMSE) funct
nrmse <- function(actual,pred){
  sqrt(ssr(actual,pred)/((length(actual)-1)*var(actual)))
} 

##percentage of outperforming direct sample mean (sample expected value)
pcorrect<- function(actual,pred){
  (1-nrmse(actual,pred))*100
}
```

# Exercise 1

![](images/Ex1.png)

We read the time series data, extract the index names and codes and separate the market indexes from the volatility indexes.

```{r}
data.env = readRDS('.\\HW1\\WorldMarkts99_20.RDS')

names<-c("India", "Brazil","UK","Germany","USA","China-Shanghai",
             "Spain","Indonesia","Mexico","Japan","Taiwan","VLIC","VIX")

markets_codes = ls(data.env)[1:11]
markets_names = names[1:11]
volatilities_codes = ls(data.env)[12:13]
volatilities_names = names[12:13]

markets = data.frame(matrix(ncol=2, nrow=11))
colnames(markets) = c('name', 'code')
markets$name = markets_names
markets$code = markets_codes

```

Next we choose the Mexico index (MXX) to compute the Exponential MOving Average for the the index time series with the indicated labmda and compare to moving window standard deviation

```{r warning=FALSE}
mex_market = xts()
mex_market = na.approx(Ad(get('MXX', data.env)))
mex_market = periodReturn(mex_market,period='daily', type='log')

lambda = 0.94
ratio = 1-lambda
n = 1
mex_ema = EMA(mex_market, n=n, ratio=ratio)

mex_sd = sd(mex_market)
ema_sd = sqrt(mean(mex_ema))

print(mex_sd)
print(ema_sd)
```
We can observe that bla bla bla

We can also graph the values below and see the results

```{r}
mex_ema_plot = dygraph(data=mex_ema, main='Exponential Moving Average') %>%
  dyRangeSelector() %>%
  dyAxis('x', label = "Date") %>%
  dyAxis('y', axisLabelFormatter='function(v){return (v).toFixed(4)}') %>%
  dyOptions(axisLabelFontSize=10, fillGraph = FALSE, fillAlpha=0.1, drawPoints=FALSE)
  
mex_ema_plot
```

# Exercise 3

![](images/Ex3_1.png)
![](images/Ex3_2.png)


To complete this tasks we need to take the following actions:

1. extract data for at daily level for  each of the 11 indexes into a time series and input an interpolated value (using na.approx function) where NAs exists in the data. As documentation indicates and this means that NA values in the time series are replaced by linear interpolation. 
2. convert the indexes time series to weekly and monthly periods
3. filter the data to contain only the windows assigned to the team (2017-07-01 - 2019-06-30)
4. Calculate the exponential moving average for all indexes and periods

The data for all indexes is stored in a list of dataframes, so we will end up with four type of dataframes combining the market indexes and volatility indexes (from EMA) with the weekly and monthly periods for each. 


```{r, warning=FALSE}
return_week = list()
return_month = list()
volat_week = list()
volat_month = list()
periods = c('weekly','monthly')

start_date="2017-07-01"
end_date="2019-06-30"

for (period in periods) {
  for (row in 1:nrow(markets)) {
    temp = xts()
    temp = na.approx(Ad(get(markets[row,'code'], data.env)))
    temp = periodReturn(temp, period=period, type='log')
    temp = window(temp, start=start_date, end=end_date)
    ema = EMA(temp, n=n, ratio=ratio)
    
    if (period == 'weekly') {
      return_week[[markets[row,'name']]] = temp
      volat_week[[markets[row,'name']]] = ema
    } else {
      return_month[[markets[row,'name']]] = temp
      volat_month[[markets[row,'name']]] = ema
    }
  }
}
```

We can now run the causality test on all the different index combinations and produce the tables indicated int he exercise. 

```{r, warning=FALSE}
gtest = function(market1, market2, minorder, maxorder){
  test_result = ""
  for (i in minorder:maxorder) {
    caus = grangertest(market1, market2, order=i)
    if (caus$`Pr(>F)`[2]<0.05) {
      test_result = paste0(test_result,"1")
    }
    else{
      test_result = paste0(test_result,"0")
    }
  }
  return(test_result)
}

res_return_week = data.frame(matrix(nrow=11, ncol=11))
colnames(res_return_week) = names(return_week)
rownames(res_return_week) = names(return_week)

res_return_month = data.frame(matrix(nrow=11, ncol=11))
colnames(res_return_month) = names(return_month)
rownames(res_return_month) = names(return_month)

res_volat_week = data.frame(matrix(nrow=11, ncol=11))
colnames(res_volat_week) = names(return_week)
rownames(res_volat_week) = names(return_week)

res_volat_month = data.frame(matrix(nrow=11, ncol=11))
colnames(res_volat_month) = names(return_month)
rownames(res_volat_month) = names(return_month)

for (first_market in names(return_week)){
  for (second_market in names(return_week)){
    if (first_market!=second_market){

      row = nrow(res_return_week)+1

      caus_week = gtest(return_week[[first_market]], return_week[[second_market]],1,4)
      res_return_week[first_market,second_market] = caus_week
      
      caus_month = gtest(return_month[[first_market]], return_month[[second_market]],1,4)
      res_return_month[first_market,second_market] = caus_month
      
      caus_week = gtest(volat_week[[first_market]], volat_week[[second_market]],1,4)
      res_volat_week[first_market,second_market] = caus_week
      
      caus_month = gtest(volat_month[[first_market]], volat_month[[second_market]],1,4)
      res_volat_month[first_market,second_market] = caus_month
      
      }
    }
}
```

```{r}
res_return_week
```

```{r}
res_return_month
```


```{r}
res_volat_week
```


```{r}
res_volat_month
```

The results indicate bla bla bla

# Exercise 5

![](images/Ex5_1.png)
![](images/Ex5_2.png)

First we read the data from the provided csv file and analyze the columns that it contains

```{r}
sp500goyal = as.xts(read.zoo('.\\data\\GoyalMonthly2005.csv', sep=',', header=TRUE, format='%Y-%m-%d'))
data=sp500goyal['1927/2005']
colnames(data)
```

Next we calculate the variables indicated for the team (ep, dp and dy) and calculate the lags for each of them, including the lags for the main index and we merge them all in the the time series dataframe. 

Given that we are calculating lags we also trim out NAs generated on by those lags calculations.


```{r}
tau = 1 

target = data$Index
target = diff(log(data$Index), diff=tau)
target = na.trim(target-mean(na.omit(target)))

sp500ep = log(data$E12) - log(stats::lag(data$Index,1)) # ep
sp500ep = diff(sp500ep, diff=tau)

sp500dp = log(data$D12) - log(stats::lag(data$Index,1)) # dp
sp500dp = diff(sp500dp, diff=tau)

sp500dy = log(data$D12) - log(stats::lag(data$Index,1)) # dy
sp500dy = diff(sp500dy, diff=tau)

feat = merge(na.trim(stats::lag(target,1)),
             na.trim(stats::lag(target,2)),
             na.trim(stats::lag(target,3)),
             sp500ep,
             na.trim(stats::lag(sp500ep,1)),
             na.trim(stats::lag(sp500ep,2)),
             sp500dp,
             na.trim(stats::lag(sp500dp,1)),
             na.trim(stats::lag(sp500dp,2)),
             sp500dy,
             na.trim(stats::lag(sp500dy,1)),
             na.trim(stats::lag(sp500dy,2)),
             #add other features here,
             all=FALSE)

dataset = merge(feat,target,all=FALSE)

colnames(dataset) = c("lag.1", "lag.2", "lag.3",
                      "ep","ep.1","ep.2",
                      "dp","dp.1","dp.2",
                      "dy","dy.1","dy.2",
                      #names of other features,
                      "TARGET")

```

We split the data in training and testing datasets using a 75/25% proportion

```{r}
##Divide data into training (75%) and testing (25%). 
T<-nrow(dataset)
p=0.75
T_trn <- round(p*T)
trainindex <- 1:T_trn
##process class sets as data frames
train_data = as.data.frame(dataset[trainindex,])
rownames(train_data) = NULL
test_data = as.data.frame(dataset[-trainindex,])
rownames(test_data) = NULL
```

And we execute our gaussian process utilizing the Radial Basis Function kernel (rbfdot). We used the RBF because the kernel is very effective in predicting interpolations and short distance extrapolations, which are good for our exercise where we predict 1 period ahead. 

```{r}
gpfit = gausspr(TARGET~., data=train_data,
                type="regression",
                kernel="rbfdot", 
                var = 0.003
)
gpfit

GPpredict <- predict(gpfit,test_data)

```


```{r}
### Evaluation of Results
actualTS = test_data[,ncol(test_data)] ##the true series to predict
predicTS = GPpredict

res <- list("GP"=pcorrect(actualTS,predicTS))
unlist(res)

mse = mean((actualTS - predicTS)^2)
mse

train.error <- error(gpfit)  
test.error <- mean((actualTS - predicTS)^2)
gap <- test.error - train.error ; gap

yl=c(min(actualTS,predicTS),max(actualTS,predicTS)) #set y limits
plot(actualTS,predicTS,ylim=yl)

plot(actualTS,t='l',col='gray20', ylab='', xlab ='',lty=3, main='GP predictions', cex.main=0.75)
lines(GPpredict,col='green',lwd=2)
legend('bottomright',legend = c('target','GP'),col=c('gray20','green'),lty=c(3,1),cex=.7)
```

The results show that our NRMSE is at 45% and 

In order to improve our gaussian process model that utilized all features, we will do some variable selection using lasso. We initially set our features and target to run them through our lasso model, where we will select our optimal lambda.

```{r}
x_vars = model.matrix(TARGET~. , dataset)[,-1]
y_var = dataset$TARGET

cv_output = cv.glmnet(x_vars, 
                      y_var,
                      alpha = 1, 
                      nfolds = 50 )


best_lam <- cv_output$lambda.min
best_lam
```

```{r}
plot(cv_output)
```


```{r}
# Rebuilding the model with best lamda value identified
lasso_best <- glmnet(x_vars, y_var, alpha = 1, lambda = best_lam)

coef(lasso_best)
```
```{r}
gpfit = gausspr(TARGET~., data=select(train_data,lag.1,dp.2,dy.2,TARGET),
                type="regression",
                kernel="rbfdot", 
                #kernel= "rbfdot", #"vanilladot",
                #kernel= ker1,  
                #kpar = list(sigma = 0.4), #list of kernel hyper-parameters for rbf
                ## if you make it constant value then does not make mle estimation of sigma
                #kpar=list(scale=2,offset=2), ##for tanh
                var = 0.003 # the initial noise variance: 0.001 default min value
)
gpfit

##build predictor (predict on test data)
GPpredict <- predict(gpfit,test_data)
```

```{r}
### Evaluation of Results
actualTS = test_data[,ncol(test_data)] ##the true series to predict
predicTS = GPpredict

res <- list("GP"=pcorrect(actualTS,predicTS))
unlist(res)

mse = mean((actualTS - predicTS)^2)
mse

##Check the gap between training error (mse) and testing error
train.error <- error(gpfit)  
test.error <- mean((actualTS - predicTS)^2)
gap <- test.error - train.error ; 
gap

##For visual comparison

yl=c(min(actualTS,predicTS),max(actualTS,predicTS)) #set y limits
plot(actualTS,predicTS,ylim=yl)

##Forecasting: PRICE, draw the simulations of price
#par( mfrow = c( 1, 2 ) )
#par(mar=c(2.5,2.5,2.5,2.5))
plot(actualTS,t='l',col='gray20', ylab='', xlab ='',lty=3, main='GP predictions', cex.main=0.75)
lines(GPpredict,col='green',lwd=2)
legend('bottomright',legend = c('target','GP'),col=c('gray20','green'),lty=c(3,1),cex=.7)
```

